{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from whole_brain_model import WholeBrainModel, ModelParams\n",
    "# from model_fitting import ModelFitting\n",
    "# from costs import Costs\n",
    "from wbm.data_loader import BOLDDataLoader\n",
    "from wbm.plotter import Plotter\n",
    "from wbm.utils import load_encoder, load_discriminator, DEVICE\n",
    "from discriminator.contrastive_discriminator import GraphEncoder, Discriminator\n",
    "from discriminator.graph_builder import GraphBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "import os, random\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Costs:\n",
    "\n",
    "    @staticmethod\n",
    "    def ledoit_wolf_shrinkage_torch(X: torch.Tensor, shrinkage_value: float = 0.1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Ledoit-Wolf shrunk covariance matrix in PyTorch (with manual shrinkage)\n",
    "        \n",
    "        Parameters:\n",
    "            X: torch.Tensor of shape (T, N) or (B, T, N)\n",
    "            shrinkage_value: float in [0, 1], amount of shrinkage\n",
    "\n",
    "        Returns:\n",
    "            shrunk_cov: torch.Tensor of shape (N, N) or (B, N, N)\n",
    "        \"\"\"\n",
    "        X = X.to(DEVICE)\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)  # (1, T, N)\n",
    "        B, T, N = X.shape\n",
    "\n",
    "        X_mean = X.mean(dim=1, keepdim=True)\n",
    "        X_centered = X - X_mean  # (B, T, N)\n",
    "        empirical_cov = torch.matmul(X_centered.transpose(1, 2), X_centered) / (T - 1)  # (B, N, N)\n",
    "\n",
    "        # Target: identity scaled by average variance\n",
    "        avg_var = torch.mean(torch.diagonal(empirical_cov, dim1=1, dim2=2), dim=1)  # (B,)\n",
    "        target = torch.stack([torch.eye(N, device=X.device) * avg_var[i] for i in range(B)], dim=0)  # (B, N, N)\n",
    "\n",
    "        shrunk_cov = (1 - shrinkage_value) * empirical_cov + shrinkage_value * target\n",
    "        return shrunk_cov.squeeze(0) if shrunk_cov.shape[0] == 1 else shrunk_cov\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute(simulated_bold, empirical_bold):\n",
    "        \"\"\"\n",
    "        Compare two BOLD time series and calcuate Pearson correlation between FC matrices\n",
    "\n",
    "        Parameters:\n",
    "            simulated_bold: torch.Tensor shape (N, T, B)\n",
    "            empirical_bold: torch.Tensor shape (N, T, B)\n",
    "\n",
    "        Returns:\n",
    "            loss: torch scalar, Pearson's correlation loss between FC matrices\n",
    "                calculated as -log(0.5 + 0.5 * global_corr)\n",
    "            root mean squared error\n",
    "            average node-wise Pearson correlation\n",
    "            average functional connectivity Pearson correlation\n",
    "        \"\"\"\n",
    "        if not isinstance(simulated_bold, torch.Tensor):\n",
    "            simulated_bold = torch.tensor(simulated_bold, dtype=torch.float32)\n",
    "        if not isinstance(empirical_bold, torch.Tensor):\n",
    "            empirical_bold = torch.tensor(empirical_bold, dtype=torch.float32)\n",
    "        \n",
    "        assert simulated_bold.shape == empirical_bold.shape, f\"Simulated and Empirical BOLD time series must have the same dimensions. Found EMP: {empirical_bold.shape}, SIM: {simulated_bold.shape}\"\n",
    "        # print(f\"Simulated BOLD shape: ({simulated_bold.shape})\")\n",
    "        N, T, B = simulated_bold.shape\n",
    "\n",
    "        rmse = torch.sqrt(torch.mean((simulated_bold - empirical_bold) ** 2))\n",
    "\n",
    "        # Compute Pearon's correlation between per node\n",
    "        rois_correlation = []\n",
    "        for b in range(B):\n",
    "            sim_batch = simulated_bold[:, :, b]\n",
    "            emp_batch = empirical_bold[:, :, b]\n",
    "\n",
    "            # Zero mean\n",
    "            s_centered = sim_batch - torch.mean(sim_batch)\n",
    "            e_centered = emp_batch - torch.mean(emp_batch)\n",
    "            \n",
    "            dot_product = (s_centered * e_centered).sum(dim=1)\n",
    "            product = (s_centered.norm(dim=1) * e_centered.norm(dim=1) + 1e-8)\n",
    "            rois_correlation.append((dot_product / product).mean().detach().item())\n",
    "\n",
    "        average_rois_correlation = float(np.mean(rois_correlation))\n",
    "\n",
    "        global_corrs = []\n",
    "        \n",
    "        for b in range(B):\n",
    "            sim_b = simulated_bold[:, :, b].permute(1, 0) # (T, N)\n",
    "            emp_b = empirical_bold[:, :, b].permute(1, 0) # (T, N)\n",
    "\n",
    "            if torch.allclose(sim_b.std(dim=0), torch.zeros_like(sim_b[0]), atol=1e-5) or \\\n",
    "            torch.allclose(emp_b.std(dim=0), torch.zeros_like(emp_b[0]), atol=1e-5):\n",
    "                print(f\"[WARNING] Batch {b}: constant or near-zero signal in sim or emp BOLD.\")\n",
    "                global_corr_b = torch.tensor(0.0, device=sim_b.device, requires_grad=True)\n",
    "                global_corrs.append(global_corr_b)\n",
    "                continue\n",
    "\n",
    "\n",
    "            # MARK: Ledoit-Wolf shrunk covariance estimation\n",
    "            # cov_sim = Costs.ledoit_wolf_shrinkage_torch(sim_b, shrinkage_value=0.1)  # (N, N)\n",
    "            # cov_emp = Costs.ledoit_wolf_shrinkage_torch(emp_b, shrinkage_value=0.1)\n",
    "            # Compute global FC matrices\n",
    "            sim_n = sim_b - torch.mean(sim_b, dim=1, keepdim=True)\n",
    "            emp_n = emp_b - torch.mean(emp_b, dim=1, keepdim=True)\n",
    "            cov_sim = sim_n @ sim_n.t()  # (N, N)\n",
    "            cov_emp = emp_n @ emp_n.t()  # (N, N)\n",
    "            std_sim = torch.sqrt(torch.diag(cov_sim) + 1e-8)\n",
    "            std_emp = torch.sqrt(torch.diag(cov_emp) + 1e-8)\n",
    "            FC_sim = cov_sim / (std_sim.unsqueeze(1) * std_sim.unsqueeze(0) + 1e-8)\n",
    "            FC_emp = cov_emp / (std_emp.unsqueeze(1) * std_emp.unsqueeze(0) + 1e-8)\n",
    "            \n",
    "            # Extract lower triangular parts (excluding the diagonal)\n",
    "            mask = torch.tril(torch.ones_like(FC_sim), diagonal=-1).bool()\n",
    "\n",
    "            sim_vec = FC_sim[mask]\n",
    "            emp_vec = FC_emp[mask]\n",
    "            sim_vec = sim_vec - torch.mean(sim_vec)\n",
    "            emp_vec = emp_vec - torch.mean(emp_vec)\n",
    "\n",
    "            dot = torch.sum(sim_vec * emp_vec)\n",
    "            norm_sim = torch.sqrt(torch.sum(sim_vec ** 2)).clamp(min=1e-6)\n",
    "            norm_emp = torch.sqrt(torch.sum(emp_vec ** 2)).clamp(min=1e-6)\n",
    "            l2_product = norm_sim * norm_emp\n",
    "\n",
    "            global_corr_b = dot / l2_product\n",
    "\n",
    "            # Skip NaNs or Infs\n",
    "            if not torch.isfinite(global_corr_b):\n",
    "                print(f\"[WARNING] NaN or Inf in global_corr_b at batch {b}\")\n",
    "                global_corr_b = torch.tensor(0.0, device=sim_vec.device, requires_grad=True)\n",
    "\n",
    "            global_corrs.append(global_corr_b)\n",
    "\n",
    "        global_corr = torch.mean(torch.stack(global_corrs)).clamp(min=1e-8)\n",
    "        global_corr = torch.nan_to_num(global_corr, nan=0.0, posinf=0.0, neginf=0.0) # NAN -> 0\n",
    "        global_corr = torch.clamp(global_corr, min=-0.999, max=0.999) # Clamp to (-1, 1)\n",
    "\n",
    "        correlation_loss = -torch.log(0.5 + 0.5 * global_corr + 1e-8)\n",
    "\n",
    "        return {\n",
    "            \"loss\": correlation_loss,\n",
    "            \"rmse\": rmse.detach().cpu().numpy(),\n",
    "            \"average_rois_correlation\": average_rois_correlation,\n",
    "            \"average_fc_correlation\": global_corr.detach().cpu().numpy()\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorHook:\n",
    "    \"\"\"\n",
    "    Wraps the frozen discriminator + GraphBuilder. Encoder is frozen, Discriminator updates each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator: Discriminator, builder: GraphBuilder, lambda_start: float = 0.1, lambda_end: float = 0.1,\n",
    "                 warmup_epochs: int = 0, finetune_lr: float = 1e-4, device: str = DEVICE):\n",
    "        self.discriminator = discriminator.to(device).eval()\n",
    "        self.builder = builder\n",
    "        self.lambda_start = lambda_start\n",
    "        self.lambda_end = lambda_end\n",
    "        self.warmup = max(1, warmup_epochs)\n",
    "        self.device = device\n",
    "        self.bce_target = torch.ones(1, device=device)\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.finetune_optimizer = optim.Adam(self.discriminator.parameters(), lr=finetune_lr)\n",
    "\n",
    "        os.makedirs(\"fooling_examples\", exist_ok=True)\n",
    "\n",
    "    def _check_prob(self, t, name):\n",
    "        bad = (t < 0) | (t > 1) | torch.isnan(t)\n",
    "        if bad.any():\n",
    "            idx = bad.nonzero(as_tuple=False)[:10]  # first few offenders\n",
    "            print(f\"[DEBUG_BCE] {name} out of range at {idx}\")\n",
    "            print(f\"  min={t.min().item():.4g}  max={t.max().item():.4g}\")\n",
    "\n",
    "\n",
    "    def _lambda_now(self, epoch: int):\n",
    "        if epoch >= self.warmup:\n",
    "            return self.lambda_end\n",
    "        alpha = epoch / self.warmup\n",
    "        return self.lambda_start * (1 - alpha) + self.lambda_end * alpha\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, bold_chunk: torch.Tensor, sc_matrix: torch.Tensor, epoch: int):\n",
    "        \"\"\" Returns lambda-scaled BCE loss \"\"\"\n",
    "        data = self.builder.build_graph(bold_chunk, sc_matrix, label=1.0)\n",
    "        score = self.discriminator(data).view(-1) # [0, 1]\n",
    "        self._check_prob(score, \"forward\")\n",
    "        loss = self.bce(score, self.bce_target)\n",
    "        return loss * self._lambda_now(epoch)\n",
    "    \n",
    "    def finetune_discriminator(self, steps: int, batch_size: int, bold_batch: torch.Tensor, sc_matrix: torch.Tensor):\n",
    "        \"\"\" Finetunes discriminator network by sampling `batch_size` elements from fooling_examples/, along with latest empirical bold batch \"\"\"\n",
    "        fooled_paths = glob(\"fooling_examples/*.pt\")[-100:] # latest 100\n",
    "        if not fooled_paths:\n",
    "            return 0.0\n",
    "        self.discriminator.train()\n",
    "        total_ft_loss = 0.0\n",
    "        random.shuffle(fooled_paths)\n",
    "\n",
    "        for step in range(min(steps, (len(fooled_paths) - 1) // batch_size + 1)):\n",
    "            batch_paths = fooled_paths[step * batch_size : (step + 1) * batch_size]\n",
    "            if not batch_paths: break\n",
    "\n",
    "            fake_graphs = [self.builder.build_graph(torch.load(p)['bold'].to(self.device),\n",
    "                                                    torch.load(p)['sc'].to(self.device), label=0.0)\n",
    "                                                    for p in batch_paths]\n",
    "            \n",
    "            total_real = bold_batch.shape[2]\n",
    "            count_real = min(len(fake_graphs), total_real)\n",
    "            indices    = random.sample(range(total_real), count_real)\n",
    "\n",
    "            real_graphs = [self.builder.build_graph(bold_batch[:, :, i],\n",
    "                                                    sc_matrix[i], label=1.0)\n",
    "                                                    for i in indices]\n",
    "            \n",
    "            batch = Batch.from_data_list(fake_graphs + real_graphs).to(DEVICE)\n",
    "            target = torch.cat([torch.zeros(len(fake_graphs)), torch.ones(len(real_graphs))]).to(DEVICE)\n",
    "\n",
    "            predictions = self.discriminator(batch)\n",
    "            self._check_prob(predictions, \"finetune\")\n",
    "            loss_d = self.bce(predictions, target)            \n",
    "\n",
    "            self.finetune_optimizer.zero_grad(set_to_none=True)\n",
    "            loss_d.backward()\n",
    "            self.finetune_optimizer.step()\n",
    "\n",
    "            total_ft_loss += loss_d.item()\n",
    "\n",
    "        self.discriminator.eval()\n",
    "        return total_ft_loss / max(1, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParams:\n",
    "    def __init__(self):\n",
    "        ## DMF parameters\n",
    "        #  starting states taken from Griffiths et al. 2022\n",
    "        self.W_E        = 1.0               # Scale for external input to excitatory population\n",
    "        self.W_I        = 0.7               # Scale for external input to inhibitory population\n",
    "        self.I_0        = 0.382              # Constant external input\n",
    "        self.tau_E      = 100.0             # Decay time (ms) for excitatory synapses\n",
    "        self.tau_I      = 10.0              # Decay time for inhibitory synapses\n",
    "        self.gamma_E    = 0.641 / 1000.0    # Kinetic parameter for excitatory dynamics\n",
    "        self.gamma_I    = 1.0 / 1000.0      # Kinetic parameter for inhibitory dynamics\n",
    "        self.sigma_E    = 0.02              # Std. of Gaussian noise for E\n",
    "        self.sigma_I    = 0.02              # Std. of Gaussian noise for I\n",
    "        self.sigma_BOLD = 0.0               # Std. of Gaussian noise for BOLD\n",
    "\n",
    "        # Sigmoid parameters for conversion of current to firing rate:\n",
    "        self.aE     = 310.0\n",
    "        self.bE     = 125.0\n",
    "        self.dE     = 0.16\n",
    "        self.aI     = 615.0\n",
    "        self.bI     = 177.0\n",
    "        self.dI     = 0.087\n",
    "\n",
    "        # Connectivity parameters\n",
    "        self.g      = 40.0               # Global coupling (long-range)\n",
    "        self.g_EE   = 2.5                # Local excitatory self-feedback\n",
    "        self.g_IE   = 0.42               # Inhibitory-to-excitatory coupling\n",
    "        self.g_EI   = 0.42               # Excitatory-to-inhibitory coupling\n",
    "\n",
    "        ## Balloon (haemodynamic) parameters\n",
    "        self.tau_s  = 0.65\n",
    "        self.tau_f  = 0.41\n",
    "        self.tau_0  = 0.98\n",
    "        self.alpha  = 0.32\n",
    "        self.rho    = 0.34\n",
    "        self.k1     = 2.38\n",
    "        self.k2     = 2.0\n",
    "        self.k3     = 0.48\n",
    "        self.V      = 0.02              # V0 in the BOLD equation\n",
    "        self.E0     = 0.34\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Whole Brain Model integrating both DMF and Balloon\n",
    "class WholeBrainModel(nn.Module):\n",
    "    def __init__(self, params: ModelParams, input_size: int, node_size: int, batch_size: int, \n",
    "                 step_size: float, tr: float, delays_max: int):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            params: ModelParams container (attributes W_E, tau_E, gamma_E, ...)\n",
    "            input_size: Number of inptu channels (e.g. noise channels) per integration step\n",
    "            node_size: Number of nodes (ROIs)\n",
    "            batch_size: Batch size (number of parallel simulations)\n",
    "            step_size: Integration time steps (0.05s)\n",
    "            tr: TR duation (0.75s); hidden_size = tr / step_size\n",
    "            delays_max: Maximum size of delay buffer\n",
    "\n",
    "        \"\"\"\n",
    "        super(WholeBrainModel, self).__init__()\n",
    "        self.node_size = node_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_size = torch.tensor(step_size, dtype=torch.float32, device=DEVICE)\n",
    "        self.tr = tr\n",
    "        self.hidden_size = int(tr / step_size)  # number of integration steps per TR\n",
    "        self.input_size = input_size  # noise input dimension\n",
    "        self.delays_max = delays_max\n",
    "\n",
    "        self.state_size = 6  # [E, I, x, f, v, q]\n",
    "\n",
    "        # DMF parameters\n",
    "        self.W_E        = (torch.tensor(params[\"W_E\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.W_I        = (torch.tensor(params[\"W_I\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.I_0        = (torch.tensor(params[\"I_0\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.tau_E      = (torch.tensor(params[\"tau_E\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.tau_I      = (torch.tensor(params[\"tau_I\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.gamma_E    = (torch.tensor(params[\"gamma_E\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.gamma_I    = (torch.tensor(params[\"gamma_I\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.sigma_E    = (torch.tensor(params[\"sigma_E\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.sigma_I    = (torch.tensor(params[\"sigma_I\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.sigma_BOLD = (torch.tensor(params[\"sigma_BOLD\"], dtype=torch.float32, device=DEVICE))\n",
    "        \n",
    "        self.aE      = (torch.tensor(params[\"aE\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.bE      = (torch.tensor(params[\"bE\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.dE      = (torch.tensor(params[\"dE\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.aI      = (torch.tensor(params[\"aI\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.bI      = (torch.tensor(params[\"bI\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.dI      = (torch.tensor(params[\"dI\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.g       = nn.Parameter(torch.tensor(params[\"g\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.g_EE    = (torch.tensor(params[\"g_EE\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.g_IE    = (torch.tensor(params[\"g_IE\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.g_EI    = (torch.tensor(params[\"g_EI\"], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "        # Balloon (hemodynamic) parameters\n",
    "        self.tau_s   = (torch.tensor(params[\"tau_s\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.tau_f   = (torch.tensor(params[\"tau_f\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.tau_0   = (torch.tensor(params[\"tau_0\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.alpha   = (torch.tensor(params[\"alpha\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.rho     = (torch.tensor(params[\"rho\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.k1      = (torch.tensor(params[\"k1\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.k2      = (torch.tensor(params[\"k2\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.k3      = (torch.tensor(params[\"k3\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.V       = (torch.tensor(params[\"V\"], dtype=torch.float32, device=DEVICE))\n",
    "        self.E0      = (torch.tensor(params[\"E0\"], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "        print(f\"[DEBUG] Model initialized with {self.state_size} states, {len(params.__dict__)} learnable parameters, and {self.hidden_size} hidden step size\")\n",
    "\n",
    "    def generate_initial_states(self):\n",
    "        \"\"\"\n",
    "        Generates the initial state for RWW (DMF) foward function. Uses same initial states as in the Griffiths et al. code\n",
    "\n",
    "        Returns:\n",
    "            initial_state: torch.Tensor of shape (node_size, input_size, batch_size)\n",
    "        \"\"\"\n",
    "        initial_state = 0.1 * np.random.uniform(0, 1, (self.node_size, self.input_size, self.batch_size))\n",
    "        baseline = np.array([0, 0, 0, 1.1, 1.0, 1.0]).reshape(1, self.input_size, 1)\n",
    "        initial_state = initial_state + baseline\n",
    "        # state_means = initial_state.mean(axis=(0, 2))\n",
    "        # E_mean, I_mean, x_mean, f_mean, v_mean, q_mean = state_means\n",
    "        # print(f\"BASE | E={E_mean:.4f} I={I_mean:.4f} x={x_mean:.4f} f={f_mean:.4f} v={v_mean:.4f} q={q_mean:.4f}\")\n",
    "        return torch.tensor(initial_state, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    def firing_rate(self, a, b, d, current):\n",
    "        \"\"\"\n",
    "        Transformation for firing rates of excitatory and inhibitory pools\n",
    "        Takes variables a, b, current and convert into a linear equation (a * current - b) while adding a small\n",
    "        amount of noise (1e-5) while dividing that term to an exponential of itself multiplied by constant d for\n",
    "        the appropriate dimensions\n",
    "        \"\"\"\n",
    "        x = a * current - b\n",
    "        return x / (1.000 - torch.exp(-d * x) + 1e-8)\n",
    "    \n",
    "\n",
    "    def forward(self, hx: torch.Tensor, external_current: torch.Tensor, noise_in: torch.Tensor, noise_out: torch.Tensor, \\\n",
    "                delays: torch.Tensor, batched_laplacian: torch.Tensor, dist_matrices: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulate on TR chunk\n",
    "        \n",
    "        Parameters:\n",
    "            hx: Current state input, shape (node_size, 6, batch_size)\n",
    "            external_current: External current input for excitatory nodes (node_size, hidden_size, batch_size)\n",
    "            noise_in: Noise tensor for state updates, shape (node_size, hidden_size, batch_size, input_size)\n",
    "            noise_out: Noise tensor for BOLD output, shape (node_size, batch_size)\n",
    "            delays: Delay buffer for E, shape (node_size, delays_max, batch_size)\n",
    "            batched_laplacian: batched Laplacian tensor, shape (batch_size, node_size, node_size)\n",
    "            dist_matrices: batched distance tensor, representing tract lengths for excitatory delays, shape (batch_size, node_size, node_size)\n",
    "        \n",
    "        Returns:\n",
    "            state: Updated state (node_size, 6, batch_size)\n",
    "            bold: Simulated BOLD signal (node_size, batch_size)\n",
    "            delays: Updated delay buffer (node_size, delays_max, batch_size)\n",
    "        \"\"\"\n",
    "        state = hx\n",
    "        dt = self.step_size\n",
    "        ones_tensor = torch.ones_like(dt, device=DEVICE)\n",
    "        relu = torch.nn.ReLU() # ReLU module\n",
    "\n",
    "        # Loop over hidden integration steps (one TR)\n",
    "        for i in range(self.hidden_size):\n",
    "            noise_step = noise_in[:, i, :, :] # (node_size, input_size, batch_size)\n",
    "            input_current = external_current[:, i, :].unsqueeze(1) # (node_size, 1, batch_size)\n",
    "\n",
    "            # --- DMF update ---\n",
    "            E = state[:, 0:1, :] # (node_size, 1, batch_size)\n",
    "            I = state[:, 1:2, :] # (node_size, 1, batch_size)\n",
    "\n",
    "            # Delayed excitatory input\n",
    "            # Compute delay indices in integration steps\n",
    "            speed = 1.5 * ones_tensor # m/s\n",
    "            delay_seconds = dist_matrices * 0.001 / speed\n",
    "            delay_steps = (delay_seconds / self.step_size).floor().long().clamp(0, self.delays_max - 1) # (batch_size, node_size, node_size)\n",
    "\n",
    "            # Gather from delay buffer\n",
    "            hE = delays.permute(2, 1, 0) # (batch_size, delays_max, node_size)\n",
    "            E_delayed = hE.gather(dim=1, index=delay_steps) # gather along delays axis, (batch_size, node_size, node_size)\n",
    "\n",
    "            # Apply Laplacian\n",
    "            weighted_delays = batched_laplacian * E_delayed\n",
    "            summed_delays = weighted_delays.sum(dim=2) \n",
    "            connectivity_effect = summed_delays.permute(1, 0).unsqueeze(1) # (node_size, 1, batch_size)\n",
    "            \n",
    "            if i == self.hidden_size - 1:\n",
    "                Plotter.plot()\n",
    "            # print('std(connectivity) / std(E) =',connectivity_effect.std().item() / E.std().item())\n",
    "\n",
    "            I_E = relu(self.W_E * self.I_0 + self.g_EE * E + self.g * connectivity_effect - self.g_IE * I) + input_current\n",
    "            I_I = relu(self.W_I * self.I_0 + self.g_EI * E - I)\n",
    "\n",
    "            R_E = self.firing_rate(self.aE, self.bE, self.dE, I_E)\n",
    "            R_I = self.firing_rate(self.aI, self.bI, self.dI, I_I)\n",
    "\n",
    "            print(f\"R_E statistics mean {R_E.mean().item():.5f} max {R_E.max().item():.5f} min {R_E.min().item()}\")\n",
    "            print(f\"R_I statistics mean {R_I.mean().item():.5f} max {R_I.max().item():.5f} min {R_I.min().item()}\")\n",
    "\n",
    "            dE = -E / self.tau_E + (ones_tensor - E) * self.gamma_E * R_E\n",
    "            dI = -I / self.tau_I + self.gamma_I * R_I\n",
    "\n",
    "            E_noise = self.sigma_E * noise_step[:, 0:1, :] * torch.sqrt(dt) # use first channel of noise for E\n",
    "            I_noise = self.sigma_I * noise_step[:, 1:2, :] * torch.sqrt(dt) # second channel for I\n",
    "\n",
    "            E_new = torch.tanh(E + dt * dE + E_noise)\n",
    "            I_new = torch.tanh(I + dt * dI + I_noise)\n",
    "\n",
    "            # --- Balloon Update ---\n",
    "            x = state[:, 2:3, :]\n",
    "            f = state[:, 3:4, :]\n",
    "            v = state[:, 4:5, :]\n",
    "            q = state[:, 5:6, :]\n",
    "\n",
    "            dx = 1 * R_E - torch.reciprocal(self.tau_s) * x - torch.reciprocal(self.tau_f) * (f - ones_tensor)\n",
    "            df = x\n",
    "            dv = (f - torch.pow(v, torch.reciprocal(self.alpha))) * torch.reciprocal(self.tau_0)\n",
    "            dq = (f * (ones_tensor - torch.pow(ones_tensor - self.rho, torch.reciprocal(f))) * torch.reciprocal(self.rho) \\\n",
    "                   - q * torch.pow(v, torch.reciprocal(self.alpha)) * torch.reciprocal(v+1e-8)) \\\n",
    "                     * torch.reciprocal(self.tau_0)\n",
    "            \n",
    "            x_new = x + dt * dx #+ noise_step[:, 2:3, :]\n",
    "            f_new = f + dt * df #+ noise_step[:, 3:4, :]    \n",
    "            v_new = v + dt * dv #+ noise_step[:, 4:5, :]\n",
    "            q_new = q + dt * dq #+ noise_step[:, 5:6, :]\n",
    "\n",
    "            state = torch.cat([E_new, I_new, x_new, f_new, v_new, q_new], dim=1)\n",
    "\n",
    "            # Discard oldest delay value. Shape (node_size, delays_max, batch_size)\n",
    "            delays = torch.cat([E_new, delays[:, :-1, :]], dim=1)\n",
    "\n",
    "        BOLD = 100.0 * self.V * torch.reciprocal(self.E0) * (self.k1 * (ones_tensor - q_new) + \\\n",
    "                        (self.k2 * (ones_tensor - q_new * torch.reciprocal(v_new))) + \\\n",
    "                        (self.k3 * (ones_tensor - v_new)))\n",
    "        BOLD = BOLD.squeeze(1)\n",
    "        BOLD = BOLD + self.sigma_BOLD * noise_out # shape (node_size, batch_size)\n",
    "        # print(f\"BOLD avg: {BOLD.mean().item():.4f}\")\n",
    "\n",
    "        return state, BOLD, delays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelFitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anish Kochhar, Imperial College London, March 2025\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelFitting:\n",
    "    def __init__(self, model: WholeBrainModel, discriminator: DiscriminatorHook, data_loader: BOLDDataLoader, num_epochs: int, lr: float, cost_function: Costs, \\\n",
    "                 smoothing_window: int = 1, finetune_steps: int = 5, finetune_batch: int = 32, log_state: bool = False, device = DEVICE):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: WholeBrainModel instance\n",
    "            discriminator: DiscriminatorHook instance, containing all functionality to get classification loss (real vs. fake)\n",
    "            data_loader: BOLDDataLoader instance providing sample_minibatch()\n",
    "            num_epochs: Number of training epochs\n",
    "            lr: Learning rate\n",
    "            cost_function: compute() function for metrics comparision between simulated and empirical BOLD\n",
    "            smoothing_window: size of moving-average window (1 = no smoothing)\n",
    "            log_state: If True, logs the evolution of state variables over TR chunks\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.discriminator_hook = discriminator\n",
    "        self.loader = data_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.cost_function = cost_function # Costs.compute\n",
    "        self.smoothing_window = smoothing_window\n",
    "        self.log_state = log_state\n",
    "\n",
    "        self.finetune_steps = finetune_steps # per epoch\n",
    "        self.finetune_batch = finetune_batch\n",
    "\n",
    "        self.logs = { \"losses\": [], \"fc_correlation\": [], \"rmse\": [], \"roi_correlation\": [], \"hidden_states\": [], \"adv_loss\": [] }\n",
    "        self.parameters_history = { name: [] for name in [\"g\", \"g_EE\", \"g_EI\", \"g_IE\"] }\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def smooth(self, bold: torch.Tensor):\n",
    "        \"\"\" Applies moving average along time dimension (dim = 1) \"\"\"\n",
    "        if self.smoothing_window <= 1:\n",
    "            return bold\n",
    "\n",
    "        N, T, B = bold.shape\n",
    "        x = bold.permute(0, 2, 1).reshape(-1, 1, T) # (N * B, T)\n",
    "        \n",
    "        # Asymmetric pad\n",
    "        left_pad = (self.smoothing_window - 1) // 2\n",
    "        right_pad = self.smoothing_window - 1 - left_pad\n",
    "        x = F.pad(x, (left_pad, right_pad), mode='replicate')\n",
    "        smoothed = F.avg_pool1d(x, kernel_size=self.smoothing_window, stride=1)\n",
    "        smoothed = smoothed.reshape(N, B, T).permute(0, 2, 1)\n",
    "        return smoothed\n",
    "\n",
    "    def compute_fc(self, matrix: torch.Tensor):\n",
    "        \"\"\" Builds the FC matrix  \"\"\"\n",
    "        zero_centered = matrix - matrix.mean(dim=1, keepdim=True)\n",
    "        covariance = zero_centered @ zero_centered.T\n",
    "        std = torch.sqrt(torch.diag(covariance)).unsqueeze(0)\n",
    "        return (covariance / (std.T * std + 1e-8)).detach().cpu().numpy()\n",
    "\n",
    "    def train(self, delays_max: int = 500, batch_size: int = 20):\n",
    "        \"\"\"\n",
    "        Train the model over multiple minibatches, iterating over TR chunks for each sample\n",
    "\n",
    "        Parameters:\n",
    "            delays_max: Maximum delay stored for residual connections\n",
    "            batch_size: Minibatch size\n",
    "        \"\"\"\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        num_batches = self.loader.batched_dataset_length(batch_size) # Minibatches per epoch)\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Initial state\n",
    "            state = self.model.generate_initial_states().to(self.device)\n",
    "            delays = torch.zeros(self.model.node_size, delays_max, batch_size, device=self.device)\n",
    "\n",
    "            batch_losses = []\n",
    "            batch_fc_corrs = []\n",
    "            batch_roi_corrs = []\n",
    "            batch_rmses = []\n",
    "            batch_adv_losses = []\n",
    "            epoch_state_log = []\n",
    "\n",
    "            self.model.sigma_BOLD.data.fill_(0.)\n",
    "\n",
    "            batch_iter = tqdm(range(num_batches), desc=f\"Epochs [{epoch}/{self.num_epochs}]\", unit=\"batch\", leave=False)\n",
    "\n",
    "            for batch_index in batch_iter:\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "                empirical_bold, normalised_sc, laplacians, dist_matrices, sampled = self.loader.sample_minibatch(batch_size)\n",
    "\n",
    "                num_TRs = empirical_bold.shape[1]   # chunk_length = 50\n",
    "\n",
    "                simulated_bold_chunks = []\n",
    "\n",
    "                for tr_index in range(num_TRs):\n",
    "                    # noise_in shape: (node_size, hidden_size, batch_size, input_size) with input_size = 6\n",
    "                    noise_in = torch.randn(self.model.node_size, self.model.hidden_size, self.model.input_size, batch_size, device=self.device)\n",
    "                    # noise_in = torch.zeros_like(noise_in)\n",
    "\n",
    "                    # noise_out shape: (node_size, batch_size)\n",
    "                    noise_out = torch.randn(self.model.node_size, batch_size, device=self.device)\n",
    "                    # noise_out = torch.zeros_like(noise_out)\n",
    "\n",
    "                    external_current = torch.zeros(self.model.node_size, self.model.hidden_size, batch_size, device=self.device)\n",
    "\n",
    "                    state, bold_chunk, delays = self.model(state, external_current, noise_in, noise_out, delays, laplacians, dist_matrices)\n",
    "                    simulated_bold_chunks.append(bold_chunk)\n",
    "\n",
    "                    if self.log_state and batch_index == 0:\n",
    "                        state_means = state.mean(dim=(0, 2)).detach().cpu().numpy()\n",
    "                        if tr_index % 10 == 0:\n",
    "                            E_mean, I_mean, x_mean, f_mean, v_mean, q_mean = state_means\n",
    "                            print(f\"TR {tr_index:02d} | E={E_mean:.4f}  I={I_mean:.4f}  x={x_mean:.4f}  f={f_mean:.4f}  v={v_mean:.4f}  q={q_mean:.4f}\" )\n",
    "                        epoch_state_log.append(state_means)\n",
    "                        \n",
    "                    # if tr_index % 10 == 0:\n",
    "                    #     print(f\"SNR {((bold_chunk - noise_out).std()/noise_out.std()).item():.1f}\",\n",
    "                    #     f\"|corr(E)| {torch.corrcoef(state[:,0,:]).abs().mean():.2f}\",\n",
    "                    #     f\"|corr(BOLD)| {torch.corrcoef(bold_chunk).abs().mean():.2f}\")\n",
    "\n",
    "\n",
    "                # Stack TR chunks to form a time series: (node_size, num_TRs, batch_size)\n",
    "                simulated_bold_epoch = torch.stack(simulated_bold_chunks, dim=1)\n",
    "                smoothed_simulated_bold_epoch = self.smooth(simulated_bold_epoch)\n",
    "\n",
    "                # Compute cost\n",
    "                metrics = self.cost_function.compute(smoothed_simulated_bold_epoch, empirical_bold)\n",
    "                loss = metrics[\"loss\"]\n",
    "\n",
    "                # Compute discriminator loss\n",
    "                # total_adversarial_loss = 0.0\n",
    "                # for b in range(batch_size):\n",
    "                #     adversarial_loss = self.discriminator_hook(smoothed_simulated_bold_epoch[:, :, b],\n",
    "                #                                                normalised_sc[b], epoch)\n",
    "                #     print(f\"Subj {sampled[b]:3d} Loss {adversarial_loss.item():.4f}\")\n",
    "                #     if adversarial_loss.item() < 1e-3: # MARK Fool threshold\n",
    "                #         torch.save({'bold': smoothed_simulated_bold_epoch[:, :, b].cpu(), 'sc': normalised_sc[b].cpu()},\n",
    "                #                     f\"fooling_examples/epoch{epoch:03d}_batch{batch_index:03d}_idx{b}.pt\")\n",
    "                #         print(f\"Saved to fooling_examples/epoch{epoch:03d}_batch{batch_index:03d}_idx{b}.pt\")\n",
    "                #         # Plotter.plot_time_series(smoothed_simulated_bold_epoch[:, :, b].unsqueeze(-1), title=\"FOOLED\")\n",
    "                #     # elif adversarial_loss.item() > 2.25:\n",
    "                #     #     torch.save({'bold': smoothed_simulated_bold_epoch[:, :, b].cpu(), 'sc': normalised_sc[b].cpu()},\n",
    "                #     #                 f\"fooling_examples_n/epoch{epoch:03d}_batch{batch_index:03d}_idx{b}.pt\")\n",
    "                #     #     print(f\"Saved to fooling_examples_n/epoch{epoch:03d}_batch{batch_index:03d}_idx{b}.pt\")\n",
    "                #     #     Plotter.plot_time_series(smoothed_simulated_bold_epoch[:, :, b].unsqueeze(-1), title=\"NOT FOOLED\")\n",
    "                        \n",
    "                #     total_adversarial_loss += adversarial_loss.item()\n",
    "                \n",
    "                # loss += total_adversarial_loss\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                batch_losses.append(loss.item())\n",
    "                batch_fc_corrs.append(metrics[\"average_fc_correlation\"].item())\n",
    "                batch_roi_corrs.append(metrics[\"average_rois_correlation\"])\n",
    "                batch_rmses.append(metrics[\"rmse\"])\n",
    "                # batch_adv_losses.append(total_adversarial_loss)\n",
    "\n",
    "                batch_iter.set_postfix(\n",
    "                    loss=f\"{loss.item():.4f}\",\n",
    "                    # adv=f\"{total_adversarial_loss:.4f}\",\n",
    "                    rmse=f\"{metrics['rmse']:.4f}\",\n",
    "                    fc_corr=f\"{metrics['average_fc_correlation'].item():.4f}\"\n",
    "                )\n",
    "\n",
    "                delays = delays.detach()\n",
    "                state = state.detach()\n",
    "            \n",
    "            # Small discriminator fine-tune (with latest batch of ground-truth signals)\n",
    "            # ft_loss = self.discriminator_hook.finetune_discriminator(self.finetune_steps, self.finetune_batch, empirical_bold, normalised_sc)\n",
    "            # print(f\"[Fintune] d-loss = {ft_loss:.4f}\")\n",
    "\n",
    "            for parameter_name in self.parameters_history:\n",
    "                self.parameters_history[parameter_name].append(getattr(self.model, parameter_name).item())\n",
    "            \n",
    "            self.logs[\"losses\"].append(np.mean(batch_losses))\n",
    "            self.logs[\"fc_correlation\"].append(np.mean(batch_fc_corrs))\n",
    "            self.logs[\"roi_correlation\"].append(np.mean(batch_roi_corrs))\n",
    "            self.logs[\"rmse\"].append(np.mean(batch_rmses))\n",
    "            self.logs[\"adv_loss\"].append(np.mean(batch_adv_losses))\n",
    "\n",
    "            if self.log_state:\n",
    "                self.logs[\"hidden_states\"].append(np.stack(epoch_state_log, axis=0))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{self.num_epochs} | \"\n",
    "                f\"Loss: {self.logs['losses'][-1]:.4f} | \"\n",
    "                f\"RMSE: {self.logs['rmse'][-1]:.4f} | \"\n",
    "                f\"ROI Corr: {self.logs['roi_correlation'][-1]:.4f} | \"\n",
    "                f\"FC Corr: {self.logs['fc_correlation'][-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Plot FC matrix heatmaps for final epoch for batch 0\n",
    "            simulated_fc = self.compute_fc(smoothed_simulated_bold_epoch[:, :, 0])\n",
    "            empirical_fc = self.compute_fc(empirical_bold[:, :, 0])\n",
    "            sampled_id = sampled[0]\n",
    "            \n",
    "            Plotter.plot_functional_connectivity_heatmaps(simulated_fc, empirical_fc, sampled_id)\n",
    "\n",
    "            Plotter.plot_node_comparison(\n",
    "                empirical_bold[:, :, 0].unsqueeze(-1),\n",
    "                smoothed_simulated_bold_epoch[:, :, 0].unsqueeze(-1),\n",
    "                node_indices=list(np.random.choice(range(self.model.node_size), size=6, replace=False))\n",
    "            )\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                print(name, param.item())\n",
    "\n",
    "\n",
    "        # Final epoch visualisations\n",
    "        epochs = list(range(1, self.num_epochs + 1))\n",
    "        Plotter.plot_loss_curve(epochs, self.logs[\"losses\"])\n",
    "        Plotter.plot_fc_correlation_curve(epochs, self.logs[\"fc_correlation\"])\n",
    "        Plotter.plot_roi_correlation_curve(epochs, self.logs[\"roi_correlation\"])\n",
    "        Plotter.plot_rmse_curve(epochs, self.logs[\"rmse\"])\n",
    "\n",
    "        if self.log_state:\n",
    "            Plotter.plot_hidden_states(self.logs[\"hidden_states\"])\n",
    "\n",
    "        Plotter.plot_coupling_parameters(self.parameters_history)\n",
    "\n",
    "        # Plot FC matrix heatmaps for final epoch for batch 0\n",
    "        simulated_fc = self.compute_fc(smoothed_simulated_bold_epoch[:, :, 0])\n",
    "        empirical_fc = self.compute_fc(empirical_bold[:, :, 0])\n",
    "        sampled_id = sampled[0]\n",
    "        \n",
    "        Plotter.plot_functional_connectivity_heatmaps(simulated_fc, empirical_fc, sampled_id)\n",
    "\n",
    "        Plotter.plot_node_comparison(\n",
    "            empirical_bold[:, :, 0].unsqueeze(-1),\n",
    "            smoothed_simulated_bold_epoch[:, :, 0].unsqueeze(-1),\n",
    "            node_indices=list(np.random.choice(range(self.model.node_size), size=6, replace=False))\n",
    "        )\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name, param.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataLoader] Loaded 100 subjects.\n",
      "[DataLoader] Created 2300 chunks (chunk length = 50).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fmri_filename = \"./HCP Data/BOLD Timeseries HCP.mat\"\n",
    "dti_filename = \"./HCP Data/DTI Fibers HCP.mat\"\n",
    "distance_matrices_path = \"./HCP Data/distance_matrices/\"\n",
    "distance_matrix_path = \"./HCP Data/schaefer100_dist.npy\"\n",
    "encoder_path = \"checkpoints/encoder.pt\"\n",
    "discriminator_path = \"checkpoints/discriminator.pt\"\n",
    "\n",
    "data_loader = BOLDDataLoader(fmri_filename, dti_filename, distance_matrices_path, chunk_length=50)\n",
    "\n",
    "## Model Settings\n",
    "batch_size = 16                         # Minibatch size\n",
    "node_size = data_loader.get_node_size() # 100\n",
    "step_size = 0.001                       # Integration time step\n",
    "tr = 0.75                               # TR duration\n",
    "input_size = 6                          # Number of noise channels\n",
    "delays_max = 500                        # Maximum delay time\n",
    "\n",
    "in_dim, hidden_dim, latent_dim = node_size, 64, 32 # From discriminator.ipynb\n",
    "\n",
    "trainer_lr               = 1e-1\n",
    "trainer_epochs           = 10\n",
    "trainer_smoothing_window = 1\n",
    "finetune_lr              = 1e-4\n",
    "finetune_steps           = 5            # Number of steps taken in each finetune\n",
    "finetune_batch           = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model initialized with 6 states, 30 learnable parameters, and 10 hidden step size\n",
      "[UTILS] GraphEncoder loaded with 37664 parameters\n",
      "[UTILS] Discriminator loaded with 51298 parameters\n"
     ]
    }
   ],
   "source": [
    "params = ModelParams()\n",
    "\n",
    "model = WholeBrainModel(params, input_size, node_size, batch_size, step_size, tr, delays_max).to(DEVICE)\n",
    "\n",
    "costs = Costs()\n",
    "\n",
    "graph_builder = GraphBuilder(node_dim=node_size, use_pca=False, device=DEVICE)\n",
    "\n",
    "encoder = load_encoder(encoder_path, in_dim, hidden_dim, latent_dim).to(DEVICE).eval()\n",
    "\n",
    "discriminator = load_discriminator(discriminator_path, encoder, latent_dim).to(DEVICE).eval()\n",
    "\n",
    "discriminator_hook = DiscriminatorHook(discriminator, graph_builder, finetune_lr=finetune_lr)\n",
    "\n",
    "trainer = ModelFitting(model, discriminator_hook, data_loader, num_epochs=trainer_epochs, lr=trainer_epochs, cost_function=costs, \\\n",
    "                       smoothing_window=trainer_smoothing_window, finetune_steps=finetune_steps, finetune_batch=finetune_batch, log_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(delays_max, batch_size)\n",
    "\n",
    "print(\"[Main] Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impulse Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def coupling_impulse_test(model, g_val, n_nodes=8, n_TR=100):\n",
    "    \"\"\"\n",
    "    Stimulate node 0 with a single-TR pulse and plot mean E over time\n",
    "    for the other nodes.  Run with g=0 and a large g to see the effect.\n",
    "    \"\"\"\n",
    "\n",
    "    old_g = model.g.data.clone()\n",
    "    model.g.data = g_val           # set global coupling\n",
    "    print(f\"g_val: {g_val}\")\n",
    "\n",
    "    B = 1\n",
    "    state  = model.generate_initial_states()[:, :model.state_size, :B]\n",
    "    delays = torch.zeros(model.node_size, model.delays_max, B)\n",
    "\n",
    "    # external pulse: 1 at first hidden step for node 0\n",
    "    external = torch.zeros(model.node_size, model.hidden_size, B)\n",
    "    external[0, 0] = 1.0\n",
    "\n",
    "    E_traj = []\n",
    "    for _ in range(n_TR):\n",
    "        noise_in  = torch.zeros(model.node_size, model.hidden_size,\n",
    "                                model.input_size, B)\n",
    "        noise_out = torch.zeros(model.node_size, B)\n",
    "        state, _, delays = model(state, external, noise_in,\n",
    "                                 noise_out, delays,\n",
    "                                 batched_laplacian=torch.zeros(B, n_nodes, n_nodes),\n",
    "                                 dist_matrices=torch.zeros(B, n_nodes, n_nodes))\n",
    "        E_traj.append(state[:,0,:].squeeze())   # store E only\n",
    "        external.zero_()   # pulse only on the first TR\n",
    "    \n",
    "    model.g.data.copy_(old_g)          # restore original g\n",
    "    E_traj = torch.stack(E_traj, 1)    # (node, time)\n",
    "    plt.plot(E_traj[0].cpu(), label='stim node')\n",
    "    plt.plot(E_traj[1:].mean(0).cpu(), label='mean of others')\n",
    "    plt.title(f'Impulse response, g={g_val}')\n",
    "    plt.legend(); plt.xlabel('TR'); plt.ylabel('E')\n",
    "    plt.show()\n",
    "\n",
    "model = WholeBrainModel(params, input_size, 8, 1, step_size, tr, 20)\n",
    "\n",
    "coupling_impulse_test(model, torch.tensor(0, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model initialized with 6 states, 29 learnable parameters, and 15 hidden step size\n",
      "[STARTING RUN] - g = 400.0, g_EE = 2.5, sigma_E = 0.019999999552965164, sigma_I = 0.019999999552965164, sigma_BOLD = 0.019999999552965164\n",
      "0.34676799178123474\n",
      "0.33873772621154785\n",
      "0.3310260474681854\n",
      "0.326227605342865\n",
      "0.3212602734565735\n",
      "0.3191368877887726\n",
      "0.31693708896636963\n",
      "0.3129013180732727\n",
      "0.31192463636398315\n",
      "0.31018397212028503\n",
      "0.30846157670021057\n",
      "0.3064529299736023\n",
      "0.3059980273246765\n",
      "0.3047659993171692\n",
      "0.30457109212875366\n",
      "0.305608332157135\n",
      "0.304277241230011\n",
      "0.3027627766132355\n",
      "0.3024885952472687\n",
      "0.30187851190567017\n",
      "0.3003745973110199\n",
      "0.2987288236618042\n",
      "0.2978981137275696\n",
      "0.298061728477478\n",
      "0.2971906065940857\n",
      "0.2967502474784851\n",
      "0.30005285143852234\n",
      "0.29807692766189575\n",
      "0.2991199195384979\n",
      "0.2994721233844757\n",
      "0.30013370513916016\n",
      "0.30186471343040466\n",
      "0.3001541793346405\n",
      "0.29975783824920654\n",
      "0.2995337247848511\n",
      "0.29842454195022583\n",
      "0.2970705032348633\n",
      "0.29795894026756287\n",
      "0.2970106601715088\n",
      "0.29881367087364197\n",
      "0.298365980386734\n",
      "0.2990242540836334\n",
      "0.2989635169506073\n",
      "0.2969057857990265\n",
      "0.2968944311141968\n",
      "0.297115296125412\n",
      "0.2983211278915405\n",
      "0.2983435392379761\n",
      "0.2992969751358032\n",
      "0.2994130849838257\n",
      "0.3004269003868103\n",
      "0.2986866235733032\n",
      "0.2988273501396179\n",
      "0.29765141010284424\n",
      "0.29885002970695496\n",
      "0.30039843916893005\n",
      "0.29970428347587585\n",
      "0.29778605699539185\n",
      "0.29730159044265747\n",
      "0.2952180802822113\n",
      "0.2950422763824463\n",
      "0.29550400376319885\n",
      "0.29525282979011536\n",
      "0.2968074679374695\n",
      "0.2971133887767792\n",
      "0.297424852848053\n",
      "0.2994292974472046\n",
      "0.2994893789291382\n",
      "0.2964419424533844\n",
      "0.29578930139541626\n",
      "0.2962625026702881\n",
      "0.296906054019928\n",
      "0.29803383350372314\n",
      "0.2981468141078949\n",
      "0.2974426746368408\n",
      "0.2976726293563843\n",
      "0.29904744029045105\n",
      "0.2990143299102783\n",
      "0.2992279529571533\n",
      "0.3002603054046631\n",
      "0.2970080077648163\n",
      "0.296642541885376\n",
      "0.2955673336982727\n",
      "0.2949149012565613\n",
      "0.29437118768692017\n",
      "0.2951953411102295\n",
      "0.2956502437591553\n",
      "0.2951681613922119\n",
      "0.2949925363063812\n",
      "0.29683059453964233\n",
      "0.29687556624412537\n",
      "0.2948245704174042\n",
      "0.29256847500801086\n",
      "0.29356786608695984\n",
      "0.2938494086265564\n",
      "0.29415789246559143\n",
      "0.29299625754356384\n",
      "0.2930658459663391\n",
      "0.2944795787334442\n",
      "0.29478904604911804\n",
      "[STARTING RUN] - g = 0.0, g_EE = 2.5, sigma_E = 0.019999999552965164, sigma_I = 0.019999999552965164, sigma_BOLD = 0.019999999552965164\n",
      "0.19444036483764648\n",
      "0.18104669451713562\n",
      "0.18068328499794006\n",
      "0.1790779083967209\n",
      "0.17601723968982697\n",
      "0.1776122897863388\n",
      "0.1772739738225937\n",
      "0.17537885904312134\n",
      "0.17695581912994385\n",
      "0.1796363890171051\n",
      "0.17586426436901093\n",
      "0.1775076538324356\n",
      "0.1771497279405594\n",
      "0.17752057313919067\n",
      "0.17761386930942535\n",
      "0.18096941709518433\n",
      "0.18018388748168945\n",
      "0.17936065793037415\n",
      "0.18104149401187897\n",
      "0.18131406605243683\n",
      "0.1813073307275772\n",
      "0.1831008493900299\n",
      "0.18270745873451233\n",
      "0.1830136477947235\n",
      "0.18268552422523499\n",
      "0.17964285612106323\n",
      "0.18046295642852783\n",
      "0.18313537538051605\n",
      "0.18518425524234772\n",
      "0.1862843632698059\n",
      "0.18778325617313385\n",
      "0.18885774910449982\n",
      "0.1878151297569275\n",
      "0.19023004174232483\n",
      "0.1898794025182724\n",
      "0.1907336264848709\n",
      "0.19087395071983337\n",
      "0.19047783315181732\n",
      "0.1916293352842331\n",
      "0.19023054838180542\n",
      "0.18999557197093964\n",
      "0.1912221908569336\n",
      "0.19088111817836761\n",
      "0.1892276257276535\n",
      "0.18775111436843872\n",
      "0.19180366396903992\n",
      "0.1930059939622879\n",
      "0.1915360689163208\n",
      "0.19605697691440582\n",
      "0.1930256336927414\n",
      "0.18961197137832642\n",
      "0.1924615502357483\n",
      "0.19185180962085724\n",
      "0.19049489498138428\n",
      "0.18747131526470184\n",
      "0.1891925185918808\n",
      "0.18995660543441772\n",
      "0.1896420419216156\n",
      "0.1883525252342224\n",
      "0.18983043730258942\n",
      "0.19133521616458893\n",
      "0.19003528356552124\n",
      "0.18975478410720825\n",
      "0.19012852013111115\n",
      "0.19305281341075897\n",
      "0.19127002358436584\n",
      "0.19019916653633118\n",
      "0.18958480656147003\n",
      "0.1900506168603897\n",
      "0.19056449830532074\n",
      "0.191669762134552\n",
      "0.19290228188037872\n",
      "0.19149957597255707\n",
      "0.19078242778778076\n",
      "0.19037926197052002\n",
      "0.18957333266735077\n",
      "0.18914276361465454\n",
      "0.19018371403217316\n",
      "0.19211211800575256\n",
      "0.19244512915611267\n",
      "0.19050201773643494\n",
      "0.19086486101150513\n",
      "0.19004791975021362\n",
      "0.18797822296619415\n",
      "0.18988655507564545\n",
      "0.1887824833393097\n",
      "0.1888466477394104\n",
      "0.18784457445144653\n",
      "0.18667072057724\n",
      "0.18715433776378632\n",
      "0.18873094022274017\n",
      "0.18839888274669647\n",
      "0.19043880701065063\n",
      "0.1906861811876297\n",
      "0.19014248251914978\n",
      "0.1905028223991394\n",
      "0.19107088446617126\n",
      "0.19220393896102905\n",
      "0.19445030391216278\n",
      "0.19333888590335846\n",
      "std bold (g=400): 8.438301086425781 std bold (g=0): 1.6198691129684448\n",
      "CORR BOLD       : 0.5334692597389221 0.4423178732395172\n",
      "CORR E          : 0.9942137002944946 0.4836348593235016\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "test_batch_size = 1\n",
    "test_delays_max = 50\n",
    "params = ModelParams()\n",
    "model = WholeBrainModel(params, input_size, node_size, test_batch_size, step_size, tr, test_delays_max)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "_, laplacians, dist_matrices, _ = data_loader.sample_minibatch(test_batch_size)\n",
    "\n",
    "model_coup = copy.deepcopy(model)\n",
    "model_coup.g.data.fill_(400.)\n",
    "model_noc  = copy.deepcopy(model)\n",
    "model_noc.g.data.fill_(0.)\n",
    "\n",
    "def run(model, laplacians, dist_matrices, device):\n",
    "    print(f\"[STARTING RUN] - g = {model.g.item()}, g_EE = {model.g_EE.item()}, sigma_E = {model.sigma_E.item()}, sigma_I = {model.sigma_I.item()}, sigma_BOLD = {model.sigma_BOLD.item()}\")\n",
    "    laplacians     = laplacians.to(device)\n",
    "    dist_matrices  = dist_matrices.to(device)\n",
    "\n",
    "    \n",
    "    state = model.generate_initial_states().to(device)\n",
    "    delays = torch.zeros(model.node_size, test_delays_max, test_batch_size)\n",
    "    bold = []\n",
    "    e = []\n",
    "    for _ in range(100):\n",
    "        noise_in  = torch.randn(node_size, model.hidden_size, input_size, test_batch_size, device=device)\n",
    "        noise_out = torch.randn(node_size, test_batch_size, device=device)\n",
    "\n",
    "        ext = torch.zeros(node_size, model.hidden_size, test_batch_size, device=device)\n",
    "        state, y, delays = model(state, ext, noise_in, noise_out, delays, laplacians, dist_matrices)\n",
    "        # print('SNR =', ((y - noise_out).std() / noise_out.std()).item())\n",
    "        e.append(state[:, 0, :].detach().squeeze(1))\n",
    "        print(state[:, 0, :].mean().item())\n",
    "        bold.append(y.mean(1))\n",
    "    return torch.stack(bold), torch.stack(e)\n",
    "\n",
    "bold_400, e_400 = run(model_coup, laplacians, dist_matrices, device)\n",
    "bold_0, e_0 = run(model_noc, laplacians, dist_matrices, device)\n",
    "\n",
    "print('std bold (g=400):', bold_400.std().item(), 'std bold (g=0):', bold_0.std().item())\n",
    "print('CORR BOLD       :', torch.corrcoef(bold_400.T).abs().mean().item(),\n",
    "                             torch.corrcoef(bold_0.T).abs().mean().item())\n",
    "print('CORR E          :', torch.corrcoef(e_400).abs().mean().item(),\n",
    "                             torch.corrcoef(e_0).abs().mean().item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
