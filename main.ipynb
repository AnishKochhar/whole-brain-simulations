{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from whole_brain_model import WholeBrainModel, ModelParams\n",
    "# from model_fitting import ModelFitting\n",
    "# from costs import Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "\n",
    "    def check_for_nans(tensor, name=\"Tensor\"):\n",
    "        \"\"\"\n",
    "        Checks if a tensor contains NaNs\n",
    "        Error message or Min / Max if not\n",
    "        \"\"\"\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"[ERROR] {name} contains NaNs!\")\n",
    "        else:\n",
    "            print(f\"[DEBUG] {name} OK. Min: {tensor.min().item():.4f}, Max: {tensor.max().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    @staticmethod\n",
    "    def plot_loss_curve(epoch_indices, loss_values):\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_indices, loss_values, marker='o')\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_rmse_curve(epoch_indices, rmse_values):\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_indices, rmse_values, marker='o')\n",
    "        plt.title(\"RMSE over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roi_correlation_curve(epoch_indices, roi_corr_values):\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_indices, roi_corr_values, marker='o')\n",
    "        plt.title(\"Average ROI Correlation over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average ROI Pearson r\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_fc_correlation_curve(epoch_indices, fc_corr_values):\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_indices, fc_corr_values, marker='o')\n",
    "        plt.title(\"Average FC Correlation over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Average FC Pearson r\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_functional_connectivity_heatmaps(simulated_fc: np.ndarray, empirical_fc: np.ndarray):\n",
    "        \"\"\"\n",
    "            Plots both simulated and empirical Functional Connectivity (heatmap) on horizontal axis\n",
    "            sim_fc, emp_fc: np.ndarray\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        sns.heatmap(simulated_fc, vmin=-1, vmax=1, cmap='coolwarm', ax=axes[0])\n",
    "        axes[0].set_title(\"Simulated FC\")\n",
    "        sns.heatmap(empirical_fc, vmin=-1, vmax=1, cmap='coolwarm', ax=axes[1])\n",
    "        axes[1].set_title(\"Empirical FC\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_time_series(time_series: torch.Tensor, title: str, max_nodes: int = 6):\n",
    "        \"\"\"\n",
    "            Plots batched BOLD time series on single plot:\n",
    "            time_series: torch.Tensor, shape (N, T, B)\n",
    "        \"\"\"\n",
    "        data = time_series.detach().cpu().numpy() if isinstance(time_series, torch.Tensor) else time_series\n",
    "        N, T, B = data.shape\n",
    "        for batch_idx in range(B):\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            for node_idx in range(min(N, max_nodes)):\n",
    "                plt.plot(np.arange(T), data[node_idx, :, batch_idx], label=f\"Node {node_idx}\")\n",
    "            plt.title(f\"{title} (Batch {batch_idx})\")\n",
    "            plt.xlabel(\"TR\")\n",
    "            plt.ylabel(\"BOLD signal\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_hidden_states(hidden_state_logs: np.ndarray, state_names = ['E', 'I', 'x', 'f', 'v', 'q']):\n",
    "        \"\"\"\n",
    "            hidden_state_logs: list of `epoch` elements, each (time_points, state_size = 6)\n",
    "            Plots each of the six state variables (E, I, x, f, v, q) over TRs, with one line per epoch\n",
    "        \"\"\"\n",
    "        time_points, state_size = hidden_state_logs[0].shape\n",
    "\n",
    "        for dim in range(state_size):\n",
    "            plt.figure(figsize=(8, 3))\n",
    "            for epoch_idx, epoch_log in enumerate(hidden_state_logs):\n",
    "                plt.plot( np.arange(time_points), epoch_log[:, dim], label=f'Epoch {epoch_idx+1}' )\n",
    "            plt.title(f\"Hidden-state '{state_names[dim]}' over TRs\")\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(state_names[dim])\n",
    "            plt.legend(loc='upper right', fontsize='small')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_coupling_parameters(**parameter_history):\n",
    "        \"\"\"\n",
    "            parameter_history: dictionary of `epoch` elements each (time_points, parameter_size = 4)\n",
    "            Plots each of the four core coupling parameters (g, g_EE, g_EI, g_IE)\n",
    "        \"\"\"\n",
    "        epochs = range(1, len(parameter_history) + 1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for param_name, values in parameter_history.items():\n",
    "            plt.plot(epochs, values, marker='o', label=param_name)\n",
    "        plt.title(\"Coupling parameters over epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Parameter Value\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def plot_node_comparison(empirical_bold: torch.Tensor, simulated_bold: torch.Tensor, node_indices=None):\n",
    "        if node_indices is None:\n",
    "            node_indices = list(range(min(6, empirical_bold.shape[0])))\n",
    "        emp = empirical_bold.detach().cpu().numpy()\n",
    "        sim = simulated_bold.detach().cpu().numpy()\n",
    "        T = emp.shape[1]\n",
    "        fig, axes = plt.subplots(len(node_indices), 1, figsize=(10, 2*len(node_indices)), sharex=True)\n",
    "        for i, node in enumerate(node_indices):\n",
    "            axes[i].plot(np.arange(T), emp[node, :, 0], label=\"Empirical\")\n",
    "            axes[i].plot(np.arange(T), sim[node, :, 0], label=\"Simulated\")\n",
    "            axes[i].set_ylabel(f\"Node {node}\")\n",
    "            if i == 0:\n",
    "                axes[i].legend()\n",
    "        axes[-1].set_xlabel(\"TR\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_laplacian(subject_index: int, laplacian_matrix: np.ndarray):\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(laplacian_matrix, cmap='viridis')\n",
    "        plt.title(f\"Laplacian Heatmap (Subject {subject_index})\")\n",
    "        plt.xlabel(\"Node\")\n",
    "        plt.ylabel(\"Node\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_distance_matrix(subject_index: int, distance_matrix: np.ndarray):\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(distance_matrix, cmap='magma')\n",
    "        plt.title(f\"Distance Matrix (Subject {subject_index})\")\n",
    "        plt.xlabel(\"Node\")\n",
    "        plt.ylabel(\"Node\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, fmri_filename: str, dti_filename: str, distance_matrices_path: str, chunk_length: int = 50):\n",
    "        \"\"\"\n",
    "        Loads fMRI (BOLD) time series, Structural Connectivity matrices, and distance (delay) matrices, and splits BOLD time series into chunks\n",
    "        \"\"\"\n",
    "        self.fmri_filename = fmri_filename\n",
    "        self.dti_filename = dti_filename\n",
    "        self.distance_matrices_path = distance_matrices_path\n",
    "        self.chunk_length = chunk_length\n",
    "        self.all_bold = []      # list of BOLD arrays, each shape (node_size, num_TRs)\n",
    "        self.all_SC = []        # list of SC matrices, each shape (node_size, node_size)\n",
    "        self.all_distances = [] # list of dist_matrix, each shape (node_size, node_size)\n",
    "        self.bold_chunks = []   # list of dicts: {'subject': int, 'bold': array (node_size, chunk_length)}\n",
    "        \n",
    "        self._load_data()\n",
    "        self._split_into_chunks()\n",
    "\n",
    "    def _load_data(self):\n",
    "        fmri_mat = scipy.io.loadmat(self.fmri_filename)\n",
    "        bold_data = fmri_mat[\"BOLD_timeseries_HCP\"]    # shape (100, 1)\n",
    "        # dti_mat = scipy.io.loadmat(self.dti_filename)\n",
    "        # dti_data = dti_mat[\"DTI_fibers_HCP\"]           # shape (100, 1)\n",
    "        num_subjects = bold_data.shape[0]\n",
    "        \n",
    "        for subject in range(num_subjects):\n",
    "            bold_subject = bold_data[subject, 0]  # shape (100, 1189)\n",
    "            # dti_subject = dti_data[subject, 0]    # shape (100, 100)\n",
    "            self.all_bold.append(bold_subject)\n",
    "            \n",
    "            # SC pre-processed: symmetric, log-transform, normalise\n",
    "            sc_path = os.path.join(self.distance_matrices_path, f\"sc_norm_subj{subject}.npy\")\n",
    "            sc_norm = np.load(sc_path)\n",
    "            self.all_SC.append(sc_norm)\n",
    "\n",
    "            dist_path = os.path.join(self.distance_matrices_path, f\"subj{subject}.npy\")\n",
    "            dist_matrix = np.load(dist_path)\n",
    "            self.all_distances.append(dist_matrix)\n",
    "            \n",
    "        print(f\"[DataLoader] Loaded {num_subjects} subjects.\")\n",
    "\n",
    "    def _split_into_chunks(self):\n",
    "        self.bold_chunks = []\n",
    "        for subject, bold_subject in enumerate(self.all_bold):\n",
    "            num_TRs = bold_subject.shape[1]\n",
    "            num_chunks = num_TRs // self.chunk_length\n",
    "            for i in range(num_chunks):\n",
    "                chunk = bold_subject[:, i*self.chunk_length:(i+1)*self.chunk_length]\n",
    "                self.bold_chunks.append({\"subject\": subject, \"bold\": chunk})\n",
    "        print(f\"[DataLoader] Created {len(self.bold_chunks)} chunks (chunk length = {self.chunk_length}).\")\n",
    "\n",
    "    def sample_minibatch(self, batch_size: int):\n",
    "        sampled = random.sample(self.bold_chunks, batch_size)\n",
    "        batched_bold = []\n",
    "        batched_SC = []\n",
    "        batched_dist = []\n",
    "        batch_subjects = []\n",
    "\n",
    "        for batch_element in sampled:\n",
    "            batched_bold.append(batch_element[\"bold\"]) # (node_size, chunk_length)\n",
    "            subject = batch_element[\"subject\"]\n",
    "            batch_subjects.append(subject)\n",
    "\n",
    "            # NOTE: Test with non-Laplacian SC\n",
    "            sc_norm = self.all_SC[subject]\n",
    "            degree_matrix = np.diag(np.sum(sc_norm, axis=1))\n",
    "            laplacian = degree_matrix - sc_norm\n",
    "            batched_SC.append(laplacian)\n",
    "\n",
    "            distance_matrix = self.all_distances[subject]\n",
    "            \n",
    "            batched_dist.append(distance_matrix)\n",
    "\n",
    "            # Plotter.plot_laplacian(subject, laplacian)\n",
    "            # Plotter.plot_distance_matrix(subject, distance_matrix)\n",
    "\n",
    "        # Stack BOLD\n",
    "        batched_bold = np.stack(batched_bold, axis=-1) # (node_size, chunk_length, batch_size)\n",
    "        batched_bold = torch.tensor(batched_bold, dtype=torch.float32)\n",
    "\n",
    "        # Stack batched SC\n",
    "        batched_SC = np.stack(batched_SC, axis=0)\n",
    "        batched_SC = torch.tensor(batched_SC, dtype=torch.float32)\n",
    "\n",
    "        # Stack distance matrices\n",
    "        batched_dist = np.stack(batched_dist, axis=0)\n",
    "        batched_dist = torch.tensor(batched_dist, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return batched_bold, batched_SC, batched_dist, batch_subjects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Costs:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute(simulated_bold, empirical_bold):\n",
    "        \"\"\"\n",
    "        Compare two BOLD time series and calcuate Pearson correlation between FC matrices\n",
    "\n",
    "        Parameters:\n",
    "            simulated_bold: torch.Tensor shape (N, T, B)\n",
    "            empirical_bold: torch.Tensor shape (N, T, B)\n",
    "\n",
    "        Returns:\n",
    "            loss: torch scalar, Pearson's correlation loss between FC matrices\n",
    "                calculated as -log(0.5 + 0.5 * global_corr)\n",
    "            root mean squared error\n",
    "            average node-wise Pearson correlation\n",
    "            average functional connectivity Pearson correlation\n",
    "        \"\"\"\n",
    "        if not isinstance(simulated_bold, torch.Tensor):\n",
    "            simulated_bold = torch.tensor(simulated_bold, dtype=torch.float32)\n",
    "        if not isinstance(empirical_bold, torch.Tensor):\n",
    "            empirical_bold = torch.tensor(empirical_bold, dtype=torch.float32)\n",
    "        \n",
    "        assert simulated_bold.shape == empirical_bold.shape, f\"Simulated and Empirical BOLD time series must have the same dimensions. Found EMP: {empirical_bold.shape}, SIM: {simulated_bold.shape}\"\n",
    "        # print(f\"Simulated BOLD shape: ({simulated_bold.shape})\")\n",
    "        N, T, B = simulated_bold.shape\n",
    "\n",
    "        rmse = torch.sqrt(torch.mean((simulated_bold - empirical_bold) ** 2)).item()\n",
    "\n",
    "        # Compute Pearon's correlation between per node\n",
    "        rois_correlation = []\n",
    "        for b in range(B):\n",
    "            sim_batch = simulated_bold[:, :, b]\n",
    "            emp_batch = empirical_bold[:, :, b]\n",
    "\n",
    "            # Zero mean\n",
    "            s_centered = sim_batch - torch.mean(sim_batch)\n",
    "            e_centered = emp_batch - torch.mean(emp_batch)\n",
    "            \n",
    "            dot_product = (s_centered * e_centered).sum(dim=1)\n",
    "            product = (s_centered.norm(dim=1) * e_centered.norm(dim=1) + 1e-8)\n",
    "            rois_correlation.append((dot_product / product).mean().item())\n",
    "\n",
    "        average_rois_correlation = float(np.mean(rois_correlation))\n",
    "\n",
    "        global_corrs = []\n",
    "        \n",
    "        for b in range(B):\n",
    "            sim_b = simulated_bold[:, :, b]\n",
    "            emp_b = empirical_bold[:, :, b]\n",
    "        \n",
    "            # Compute global FC matrices\n",
    "            sim_n = sim_b - torch.mean(sim_b, dim=1, keepdim=True)\n",
    "            emp_n = emp_b - torch.mean(emp_b, dim=1, keepdim=True)\n",
    "            cov_sim = sim_n @ sim_n.t()  # (N, N)\n",
    "            cov_emp = emp_n @ emp_n.t()  # (N, N)\n",
    "            std_sim = torch.sqrt(torch.diag(cov_sim) + 1e-8)\n",
    "            std_emp = torch.sqrt(torch.diag(cov_emp) + 1e-8)\n",
    "            FC_sim = cov_sim / (std_sim.unsqueeze(1) * std_sim.unsqueeze(0) + 1e-8)\n",
    "            FC_emp = cov_emp / (std_emp.unsqueeze(1) * std_emp.unsqueeze(0) + 1e-8)\n",
    "            \n",
    "            # Extract lower triangular parts (excluding the diagonal)\n",
    "            mask = torch.tril(torch.ones_like(FC_sim), diagonal=-1).bool()\n",
    "\n",
    "            sim_vec = FC_sim[mask]\n",
    "            emp_vec = FC_emp[mask]\n",
    "            sim_vec = sim_vec - torch.mean(sim_vec)\n",
    "            emp_vec = emp_vec - torch.mean(emp_vec)\n",
    "            \n",
    "            global_corr_b = torch.sum(sim_vec * emp_vec) / (torch.sqrt(torch.sum(sim_vec**2)) * torch.sqrt(torch.sum(emp_vec**2)) + 1e-8)\n",
    "            global_corrs.append(global_corr_b)\n",
    "        \n",
    "        global_corr = torch.mean(torch.stack(global_corrs))\n",
    "\n",
    "        correlation_loss = -torch.log(0.5 + 0.5 * global_corr + 1e-8)\n",
    "        return {\n",
    "            \"loss\": correlation_loss,\n",
    "            \"rmse\": rmse,\n",
    "            \"average_rois_correlation\": average_rois_correlation,\n",
    "            \"average_fc_correlation\": global_corr.detach().cpu().numpy()\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParams:\n",
    "    def __init__(self):\n",
    "        ## DMF parameters\n",
    "        #  starting states taken from Griffiths et al. 2022\n",
    "        self.W_E    = 1.0               # Scale for external input to excitatory population\n",
    "        self.W_I    = 0.7               # Scale for external input to inhibitory population\n",
    "        self.I_0     = 0.32             # Constant external input\n",
    "        self.tau_E  = 100.0             # Decay time (ms) for excitatory synapses\n",
    "        self.tau_I  = 10.0              # Decay time for inhibitory synapses\n",
    "        self.gamma_E = 0.641 / 1000.0   # Kinetic parameter for excitatory dynamics\n",
    "\n",
    "        # Sigmoid parameters for conversion of current to firing rate:\n",
    "        self.aE     = 310.0\n",
    "        self.bE     = 125.0\n",
    "        self.dE     = 0.16\n",
    "        self.aI     = 615.0\n",
    "        self.bI     = 177.0\n",
    "        self.dI     = 0.087\n",
    "\n",
    "        # Connectivity parameters\n",
    "        self.g      = 20.0              # Global coupling (long-range)\n",
    "        self.g_EE   = 0.1               # Local excitatory self-feedback\n",
    "        self.g_IE   = 0.1               # Inhibitory-to-excitatory coupling\n",
    "        self.g_EI   = 0.1               # Excitatory-to-inhibitory coupling\n",
    "\n",
    "        ## Balloon (haemodynamic) parameters\n",
    "        self.tau_s  = 0.65\n",
    "        self.tau_f  = 0.41\n",
    "        self.tau_0  = 0.98\n",
    "        self.alpha  = 0.32\n",
    "        self.rho    = 0.34\n",
    "        self.k1     = 2.38\n",
    "        self.k2     = 2.0\n",
    "        self.k3     = 0.48\n",
    "        self.V      = 0.02      # V0 in the BOLD equation\n",
    "        self.E0     = 0.34\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Whole Brain Model integrating both DMF and Balloon\n",
    "class WholeBrainModel(nn.Module):\n",
    "    def __init__(self, params: ModelParams, input_size: int, node_size: int, batch_size: int, \n",
    "                 step_size: float, tr: float):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            params: ModelParams container (attributes W_E, tau_E, gamma_E, ...)\n",
    "            input_size: Number of inptu channels (e.g. noise channels) per integration step\n",
    "            node_size: Number of nodes (ROIs)\n",
    "            batch_size: Batch size (number of parallel simulations)\n",
    "            step_size: Integration time steps (0.05s)\n",
    "            tr: TR duation (0.75s); hidden_size = tr / step_size\n",
    "\n",
    "        \"\"\"\n",
    "        super(WholeBrainModel, self).__init__()\n",
    "        self.node_size = node_size\n",
    "        self.batch_size = batch_size\n",
    "        self.step_size = step_size\n",
    "        self.tr = tr\n",
    "        self.hidden_size = int(tr / step_size)  # number of integration steps per TR\n",
    "        self.input_size = input_size  # noise input dimension\n",
    "\n",
    "        self.state_size = 6  # [E, I, x, f, v, q]\n",
    "\n",
    "        # DMF parameters\n",
    "        self.W_E     = nn.Parameter(torch.tensor(params[\"W_E\"], dtype=torch.float32))\n",
    "        self.W_I     = nn.Parameter(torch.tensor(params[\"W_I\"], dtype=torch.float32))\n",
    "        self.I_0     = nn.Parameter(torch.tensor(params[\"I_0\"], dtype=torch.float32))\n",
    "        self.tau_E   = nn.Parameter(torch.tensor(params[\"tau_E\"], dtype=torch.float32))\n",
    "        self.tau_I   = nn.Parameter(torch.tensor(params[\"tau_I\"], dtype=torch.float32))\n",
    "        self.gamma_E = nn.Parameter(torch.tensor(params[\"gamma_E\"], dtype=torch.float32))\n",
    "        self.aE      = nn.Parameter(torch.tensor(params[\"aE\"], dtype=torch.float32))\n",
    "        self.bE      = nn.Parameter(torch.tensor(params[\"bE\"], dtype=torch.float32))\n",
    "        self.dE      = nn.Parameter(torch.tensor(params[\"dE\"], dtype=torch.float32))\n",
    "        self.aI      = nn.Parameter(torch.tensor(params[\"aI\"], dtype=torch.float32))\n",
    "        self.bI      = nn.Parameter(torch.tensor(params[\"bI\"], dtype=torch.float32))\n",
    "        self.dI      = nn.Parameter(torch.tensor(params[\"dI\"], dtype=torch.float32))\n",
    "        self.g       = nn.Parameter(torch.tensor(params[\"g\"], dtype=torch.float32))\n",
    "        self.g_EE    = nn.Parameter(torch.tensor(params[\"g_EE\"], dtype=torch.float32))\n",
    "        self.g_IE    = nn.Parameter(torch.tensor(params[\"g_IE\"], dtype=torch.float32))\n",
    "        self.g_EI    = nn.Parameter(torch.tensor(params[\"g_EI\"], dtype=torch.float32))\n",
    "\n",
    "        # Balloon (hemodynamic) parameters\n",
    "        self.tau_s   = nn.Parameter(torch.tensor(params[\"tau_s\"], dtype=torch.float32))\n",
    "        self.tau_f   = nn.Parameter(torch.tensor(params[\"tau_f\"], dtype=torch.float32))\n",
    "        self.tau_0   = nn.Parameter(torch.tensor(params[\"tau_0\"], dtype=torch.float32))\n",
    "        self.alpha   = nn.Parameter(torch.tensor(params[\"alpha\"], dtype=torch.float32))\n",
    "        self.rho     = nn.Parameter(torch.tensor(params[\"rho\"], dtype=torch.float32))\n",
    "        self.k1      = nn.Parameter(torch.tensor(params[\"k1\"], dtype=torch.float32))\n",
    "        self.k2      = nn.Parameter(torch.tensor(params[\"k2\"], dtype=torch.float32))\n",
    "        self.k3      = nn.Parameter(torch.tensor(params[\"k3\"], dtype=torch.float32))\n",
    "        self.V       = nn.Parameter(torch.tensor(params[\"V\"], dtype=torch.float32))\n",
    "        self.E0      = nn.Parameter(torch.tensor(params[\"E0\"], dtype=torch.float32))\n",
    "\n",
    "        print(f\"[DEBUG] Model initialized with {self.state_size} states, {len(params.__dict__)} learnable parameters, and {self.hidden_size} hidden step size\")\n",
    "\n",
    "    def generate_initial_states(self):\n",
    "        \"\"\"\n",
    "        Generates the initial state for RWW (DMF) foward function. Uses same initial states as in the Griffiths et al. code\n",
    "\n",
    "        Returns:\n",
    "            initial_state: torch.Tensor of shape (node_size, input_size, batch_size)\n",
    "        \"\"\"\n",
    "        initial_state = 0.45 * np.random.uniform(0, 1, (self.node_size, self.input_size, self.batch_size))\n",
    "        baseline = np.array([0, 0, 0, 1.0, 1.0, 1.0]).reshape(1, self.input_size, 1)\n",
    "        initial_state = initial_state + baseline\n",
    "        return torch.tensor(initial_state, dtype=torch.float32)\n",
    "\n",
    "    def h_tf(self, a, b, d, current):\n",
    "        \"\"\"\n",
    "        Transformation for firing rates of excitatory and inhibitory pools\n",
    "        Takes variables a, b, current and convert into a linear equation (a * current - b) while adding a small\n",
    "        amount of noise (1e-5) while dividing that term to an exponential of itself multiplied by constant d for\n",
    "        the appropriate dimensions\n",
    "        \"\"\"\n",
    "        num = 1e-5 + torch.abs(a * current - b)\n",
    "        den = 1e-5 * d + torch.abs(1.000 - torch.exp(-d * (a * current - b)))\n",
    "        return torch.divide(num, den + 1e-8)\n",
    "    \n",
    "\n",
    "    def forward(self, hx: torch.Tensor, noise_in: torch.Tensor, noise_out: torch.Tensor, delays: torch.Tensor, \\\n",
    "                 batched_laplacian: torch.Tensor, dist_matrices: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Simulate on TR chunk\n",
    "        \n",
    "        Parameters:\n",
    "            hx: Current state input, shape (node_size, 6, batch_size)\n",
    "            noise_in: Noise tensor for state updates, shape (node_size, hidden_size, batch_size, input_size)\n",
    "            noise_out: Noise tensor for BOLD output, shape (node_size, batch_size)\n",
    "            delays: Delay buffer for E, shape (node_size, delays_max, batch_size)\n",
    "            batched_laplacian: batched Laplacian tensor, shape (batch_size, node_size, node_size)\n",
    "            dist_matrices: batched distance tensor, representing tract lengths for excitatory delays, shape (batch_size, node_size, node_size)\n",
    "        \n",
    "        Returns:\n",
    "            state: Updated state (node_size, 6, batch_size)\n",
    "            bold: Simulated BOLD signal (node_size, batch_size)\n",
    "            delays: Updated delay buffer (node_size, delays_max, batch_size)\n",
    "        \"\"\"\n",
    "        dt = self.step_size\n",
    "        state = hx\n",
    "\n",
    "        # Loop over hidden integration steps (one TR)\n",
    "        for i in range(self.hidden_size):\n",
    "            noise_step = noise_in[:, i, :, :] # (node_size, input_size, batch_size)\n",
    "\n",
    "            # --- DMF update ---\n",
    "            E = state[:, 0:1, :] # (node_size, 1, batch_size)\n",
    "            I = state[:, 1:2, :] # (node_size, 1, batch_size)\n",
    "\n",
    "            # Delayed excitatory input\n",
    "            # Compute delay indices in integration steps\n",
    "            speed = 1.5 # m/s\n",
    "            delay_seconds = dist_matrices * 0.001 / speed\n",
    "            delay_steps = (delay_seconds / self.step_size).floor().long()\n",
    "\n",
    "            # Gather from delay buffer\n",
    "            hE = delays.permute(2, 0, 1) # (batch_size, node_size, delays_max)\n",
    "            E_delayed = hE.gather(dim=2, index=delay_steps) # gather along delays axis, (batch_size, node_size, node_size)\n",
    "            E_delayed_post = E_delayed.permute(0, 2, 1) # transpose to j->i indexing\n",
    "\n",
    "            # Apply Laplacian\n",
    "            connectivity_b = torch.bmm(batched_laplacian, E_delayed_post) # (batch_size, node_size, 1)\n",
    "            connectivity_effect = connectivity_b.permute(1, 2, 0)\n",
    "\n",
    "            # for b, g in enumerate(connectivity_b):\n",
    "            #     Plotter.plot_distance_matrix(b, g)\n",
    "\n",
    "\n",
    "            I_E = torch.nn.functional.relu(self.W_E * self.I_0 + self.g_EE * E + self.g * connectivity_effect - self.g_IE * I)\n",
    "            I_I = torch.nn.functional.relu(self.W_I * self.I_0 + self.g_EI * E - I)\n",
    "\n",
    "            R_E = self.h_tf(self.aE, self.bE, self.dE, I_E)\n",
    "            R_I = self.h_tf(self.aI, self.bI, self.dI, I_I)\n",
    "\n",
    "            dE = -E / self.tau_E + (1 - E) * self.gamma_E * R_E\n",
    "            dI = -I / self.tau_I + R_I\n",
    "\n",
    "            E_new = torch.tanh(torch.nn.functional.relu(E + dt * dE + noise_step[:, 0:1, :]))  # use first channel of noise for E\n",
    "            I_new = torch.tanh(torch.nn.functional.relu(I + dt * dI + noise_step[:, 1:2, :]))  # second channel for I\n",
    "\n",
    "            # --- Balloon Update ---\n",
    "            x = state[:, 2:3, :]\n",
    "            f = state[:, 3:4, :]\n",
    "            v = state[:, 4:5, :]\n",
    "            q = state[:, 5:6, :]\n",
    "\n",
    "            dx = E_new - torch.reciprocal(self.tau_s) * x - torch.reciprocal(self.tau_f) * (f - 1)\n",
    "            df = x\n",
    "            dv = (f - torch.pow(v, torch.reciprocal(self.alpha))) * torch.reciprocal(self.tau_0)\n",
    "            dq = (f * (1 - torch.pow(1 - self.rho, torch.reciprocal(f))) * torch.reciprocal(self.rho) \\\n",
    "                   - q * torch.pow(v, torch.reciprocal(self.alpha)) * torch.reciprocal(v+1e-8)) \\\n",
    "                     * torch.reciprocal(self.tau_0)\n",
    "            \n",
    "            x_new = x + dt * dx + noise_step[:, 2:3, :]\n",
    "            f_new = f + dt * df + noise_step[:, 3:4, :]\n",
    "            v_new = v + dt * dv + noise_step[:, 4:5, :]\n",
    "            q_new = q + dt * dq + noise_step[:, 5:6, :]\n",
    "\n",
    "            state = torch.cat([E_new, I_new, x_new, f_new, v_new, q_new], dim=1)\n",
    "\n",
    "            # Discard oldest delay value. Shape (node_size, delays_max, batch_size)\n",
    "            delays = torch.cat([E_new, delays[:, :-1, :]], dim=1)\n",
    "\n",
    "        BOLD = self.V * (self.k1 * (1 - q_new) + \\\n",
    "                        (self.k2 * (1 - q_new * torch.reciprocal(v_new))) + \\\n",
    "                        (self.k3 * (1 - v_new)))\n",
    "        BOLD = BOLD.squeeze(1)\n",
    "        BOLD = BOLD + noise_out\n",
    "\n",
    "        # print(f\"[DEBUG] BOLD shape: {BOLD.shape} (expected ({self.node_size}, {self.batch_size}))\")\n",
    "\n",
    "        return state, BOLD, delays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelFitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anish Kochhar, Imperial College London, March 2025\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelFitting:\n",
    "    def __init__(self, model: WholeBrainModel, data_loader: DataLoader, num_epochs: int, lr: float, cost_function: Costs, smoothing_window: int = 1, log_state: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            model: WholeBrainModel instance\n",
    "            data_loader: DataLoader instance providing sample_minibatch()\n",
    "            num_epochs: Number of training epochs\n",
    "            lr: Learning rate\n",
    "            cost_function: compute() function for metrics comparision between simulated and empirical BOLD\n",
    "            smoothing_window: size of moving-average window (1 = no smoothing)\n",
    "            log_state: If True, logs the evolution of state variables over TR chunks\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loader = data_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.cost_function = cost_function # Costs.compute\n",
    "        self.smoothing_window = smoothing_window\n",
    "        self.log_state = log_state\n",
    "\n",
    "        self.logs = { \"losses\": [], \"fc_corr\": [], \"rmse\": [], \"roi_corr\": [], \"hidden_states\": [] }\n",
    "        self.parameters_history = { name: [] for name in [\"g\", \"g_EE\", \"g_EI\", \"g_IE\"] }\n",
    "        # self.hidden_state_logs = [] # List per epoch (time x state_means)\n",
    "\n",
    "    def smooth(self, bold: torch.Tensor):\n",
    "        \"\"\" Applies moving average along time dimension (dim = 1) \"\"\"\n",
    "        if self.smoothing_window <= 1:\n",
    "            return bold\n",
    "        # kernel = torch.ones(self.smoothing_window, dtype=bold.dtype, device=bold.device) / self.smoothing_window\n",
    "        N, T, B = bold.shape\n",
    "        x = bold.permute(0, 2, 1).reshape(-1, 1, T) # (N * B, T)\n",
    "        # Asymmetric Pad\n",
    "        left_pad = (self.smoothing_window - 1) // 2\n",
    "        right_pad = self.smoothing_window - 1 - left_pad\n",
    "        x = F.pad(x, (left_pad, right_pad), mode='replicate')\n",
    "        smoothed = F.avg_pool1d(x, kernel_size=self.smoothing_window, stride=1)\n",
    "        smoothed = smoothed.reshape(N, B, T).permute(0, 2, 1)\n",
    "        return smoothed\n",
    "\n",
    "    def compute_fc(self, matrix: torch.Tensor):\n",
    "        \"\"\" Builds the FC matrix  \"\"\"\n",
    "        zero_centered = matrix - matrix.mean(dim=1, keepdim=True)\n",
    "        covariance = zero_centered @ zero_centered.T\n",
    "        std = torch.sqrt(torch.diag(covariance)).unsqueeze(0)\n",
    "        return (covariance / (std.T * std + 1e-8)).detach().cpu().numpy()\n",
    "\n",
    "    def train(self, delays_max: int = 500, batch_size: int = 20):\n",
    "        \"\"\"\n",
    "        Train the model over multiple minibatches, iterating over TR chunks for each sample\n",
    "\n",
    "        Parameters:\n",
    "            delays_max: Maximum delay stored for residual connections\n",
    "            batch_size: Minibatch size\n",
    "        \"\"\"\n",
    "\n",
    "        state = self.model.generate_initial_states()\n",
    "        delays = torch.zeros(self.model.node_size, delays_max, batch_size)\n",
    "\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            empirical_bold, laplacians, dist_matrices, _sampled = self.loader.sample_minibatch(batch_size)\n",
    "            num_TRs = empirical_bold.shape[1]\n",
    "\n",
    "            simulated_bold_chunks = []\n",
    "            epoch_state_log = []\n",
    "\n",
    "            for _tr in range(num_TRs):\n",
    "                # -- Generate noise for integration steps --\n",
    "                # noise_in shape: (node_size, hidden_size, batch_size, input_size) with input_size = 6\n",
    "                noise_in = torch.randn(self.model.node_size, self.model.hidden_size, self.model.input_size, batch_size, device=empirical_bold.device) * 0.02\n",
    "                noise_in = torch.zeros_like(noise_in)\n",
    "                # noise_out shape: (node_size, batch_size)\n",
    "                noise_out = torch.randn(self.model.node_size, batch_size, device=empirical_bold.device) * 0.02\n",
    "                noise_out = torch.zeros_like(noise_out)\n",
    "\n",
    "                state, bold_chunk, delays = self.model(state, noise_in, noise_out, delays, laplacians, dist_matrices)\n",
    "                simulated_bold_chunks.append(bold_chunk)\n",
    "\n",
    "                if self.log_state:\n",
    "                    epoch_state_log.append(state.mean(dim=(0, 2)).detach().cpu().numpy())\n",
    "\n",
    "            # Stack TR chunks to form a time series: (node_size, num_TRs, batch_size)\n",
    "            simulated_bold_epoch = torch.stack(simulated_bold_chunks, dim=1)\n",
    "            smoothed_simulated_bold_epoch = self.smooth(simulated_bold_epoch)\n",
    "\n",
    "            # Compute cost \n",
    "            metrics = self.cost_function.compute(smoothed_simulated_bold_epoch, empirical_bold)\n",
    "            metrics[\"loss\"].backward()\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(name, param.grad.norm().item())\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            for parameter_name in self.parameters_history:\n",
    "                self.parameters_history[parameter_name].append(getattr(self.model, parameter_name).item())\n",
    "            \n",
    "            self.logs[\"losses\"].append(metrics[\"loss\"].detach().cpu().numpy())\n",
    "            self.logs[\"fc_corr\"].append(metrics[\"average_fc_correlation\"])\n",
    "            self.logs[\"roi_corr\"].append(metrics[\"average_rois_correlation\"])\n",
    "            self.logs[\"rmse\"].append(metrics[\"rmse\"])\n",
    "\n",
    "            if self.log_state:\n",
    "                self.logs[\"hidden_states\"].append(np.stack(epoch_state_log, axis=0))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{self.num_epochs} | \"\n",
    "                f\"Loss: {metrics['loss'].item():.4f} | \"\n",
    "                f\"RMSE: {metrics['rmse']:.4f} | \"\n",
    "                f\"Avg ROI Corr: {metrics['average_rois_correlation']:.4f} | \"\n",
    "                f\"Avg FC Corr: {metrics['average_fc_correlation']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Final epoch visualisations\n",
    "        epochs = list(range(1, self.num_epochs + 1))\n",
    "        Plotter.plot_loss_curve(epochs, self.logs[\"losses\"])\n",
    "        Plotter.plot_fc_correlation_curve(epochs, self.logs[\"fc_corr\"])\n",
    "        Plotter.plot_roi_correlation_curve(epochs, self.logs[\"roi_corr\"])\n",
    "        Plotter.plot_rmse_curve(epochs, self.logs[\"rmse\"])\n",
    "\n",
    "        if self.log_state:\n",
    "            Plotter.plot_hidden_state_evolution(self.logs[\"hidden_states\"])\n",
    "\n",
    "        Plotter.plot_coupling_parameters(self.parameters_history)\n",
    "\n",
    "        # Plot FC matrix heatmaps for final epoch for batch 0\n",
    "        simulated_fc = self.compute_fc(smoothed_simulated_bold_epoch[:, :, 0])\n",
    "        empirical_fc = self.compute_fc(empirical_bold[:, :, 0])\n",
    "        \n",
    "        Plotter.plot_functional_connectivity_heatmaps(simulated_fc, empirical_fc)\n",
    "\n",
    "        Plotter.plot_node_comparison(\n",
    "            empirical_bold[:, :, 0].unsqueeze(-1),\n",
    "            smoothed_simulated_bold_epoch[:, :, 0].unsqueeze(-1),\n",
    "            node_indices=list(range(6))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataLoader] Loaded 100 subjects.\n",
      "[DataLoader] Created 2300 chunks (chunk length = 50).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fmri_filename = \"../HCP Data/BOLD Timeseries HCP.mat\"\n",
    "dti_filename = \"../HCP Data/DTI Fibers HCP.mat\"\n",
    "distance_matrices_path = \"../HCP Data/distance_matrices/\"\n",
    "\n",
    "\n",
    "data_loader = DataLoader(fmri_filename, dti_filename, distance_matrices_path, chunk_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8                         # Minibatch size\n",
    "_, sc, dist, _ = data_loader.sample_minibatch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Settings\n",
    "node_size = sc.shape[1]                 # 100\n",
    "step_size = 0.05                        # Integration time step\n",
    "tr = 0.75                               # TR duration\n",
    "input_size = 6                          # Number of noise channels\n",
    "delays_max = 20                         # Maximum delay time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model initialized with 6 states, 26 learnable parameters, and 15 hidden step size\n"
     ]
    }
   ],
   "source": [
    "params = ModelParams()\n",
    "\n",
    "model = WholeBrainModel(params, input_size, node_size, batch_size, step_size, tr)\n",
    "\n",
    "costs = Costs()\n",
    "\n",
    "trainer = ModelFitting(model, data_loader, num_epochs=10, lr=0.001, cost_function=costs, smoothing_window=10, log_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train(delays_max, batch_size)\n",
    "\n",
    "print(\"[Main] Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
